---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

```{r}
#| label: setup-packages
#| echo: false
#| message: false
#| warning: false

# Install packages if not already installed
required_packages <- c("tidyverse", "randomForest", "gridExtra", "rpart")
packages_to_install <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]

if(length(packages_to_install) > 0) {
  install.packages(packages_to_install, repos = "https://cloud.r-project.org")
}
```

# üå≤ Random Forest Challenge - The Power of Weak Learners

::: {.callout-important}
## üìä Challenge Requirements In [Student Analysis Section](#student-analysis-section)

Navigate to the [Student Analysis Section](#student-analysis-section) to see the challenge requirements.

:::


## The Problem: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance **for this dataset**.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## Data and Methodology

We analyze the Ames Housing dataset, which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is ideal for our analysis because:

- **Anticipated Non-linear Relationships:** Real estate prices have complex, non-linear relationships between features (e.g., square footage in wealthy vs. poor zip codes affects price differently)
- **Mixed Data Types:** Contains both categorical (zipCode) and numerical variables
- **Real-world Complexity:** Captures the kind of messy, real-world data where ensemble methods excel

Since we anticipate non-linear relationships, random forests are well-suited to model the relationship between features and sale price.

```{r}
#| label: load-and-model-r
#| echo: true
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
set.seed(123)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3)
set.seed(123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3)
set.seed(123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3)
set.seed(123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3)
set.seed(123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3)
set.seed(123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3)
set.seed(123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3)
set.seed(123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3)
```

## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### Performance Trends

```{r}
#| label: performance-comparison-r
#| echo: true
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```

## Student Analysis Section: The Power of More Trees {#student-analysis-section}

```{r}
#| label: rmse-plot
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

library(ggplot2)

# RMSE Plot
ggplot(performance_df) +
  geom_line(aes(x = Trees, y = RMSE_Test, color = "Test"), size = 1.2) +
  geom_point(aes(x = Trees, y = RMSE_Test, color = "Test"), size = 3) +
  geom_line(aes(x = Trees, y = RMSE_Train, color = "Training"), size = 1.2) +
  geom_point(aes(x = Trees, y = RMSE_Train, color = "Training"), size = 3) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_color_manual(values = c("Test" = "#E74C3C", "Training" = "#3498DB")) +
  labs(
    title = "RMSE vs Number of Trees",
    x = "Number of Trees (log scale)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank()
  )
```

```{r}
#| label: r2-plot
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# R-squared Plot
ggplot(performance_df) +
  geom_line(aes(x = Trees, y = R_squared), color = "#27AE60", size = 1.2) +
  geom_point(aes(x = Trees, y = R_squared), color = "#27AE60", size = 3) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  labs(
    title = "R¬≤ vs Number of Trees",
    x = "Number of Trees (log scale)",
    y = "R¬≤ (Test Data)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank()
  )
```

#### Discussion: Your Analysis Here
  The most dramatic improvement in RMSE performance occurs between 1 and 5 trees. This is quite a dramamtic change as it improves the RMSE by about 35%. There is still a noticable improvement in performance between 5 and 25 trees; however, after 25 trees, the improvement is minimal if any at all. There is a similar pattern in the R-squared plot with a large improvement between 1 and 5 trees, and a minimal improvement after 25 trees. After around 100 trees, the improvement is only around 1.5%. This suggests that for most practical applications, 100-500 trees is more than enough to achieve a good balance between performance and computational cost.


### 2. Overfitting Visualization and Analysis

```{r}
#| label: overfitting-comparison
#| echo: false
#| message: false
#| warning: false
#| fig-width: 14
#| fig-height: 6

library(rpart)
library(gridExtra)

# Train decision trees with different max depths
depths <- c(1, 3, 5, 10, 15, 20, 25, 30)
dt_results <- data.frame(
  Depth = integer(),
  RMSE_Train = numeric(),
  RMSE_Test = numeric()
)

for (depth in depths) {
  set.seed(123)
  dt_model <- rpart(SalePrice ~ ., data = train_data, 
                    control = rpart.control(maxdepth = depth, cp = 0))
  
  pred_train <- predict(dt_model, train_data)
  pred_test <- predict(dt_model, test_data)
  
  rmse_train <- sqrt(mean((train_data$SalePrice - pred_train)^2))
  rmse_test <- sqrt(mean((test_data$SalePrice - pred_test)^2))
  
  dt_results <- rbind(dt_results, data.frame(
    Depth = depth,
    RMSE_Train = rmse_train,
    RMSE_Test = rmse_test
  ))
}

# Random Forest subset for comparison
rf_subset <- performance_df[performance_df$Trees %in% c(1, 5, 25, 100, 500, 1000), ]

# Get y-axis limits to use for both plots (ensures alignment)
y_min <- min(c(dt_results$RMSE_Train, dt_results$RMSE_Test, 
               rf_subset$RMSE_Train, rf_subset$RMSE_Test))
y_max <- max(c(dt_results$RMSE_Train, dt_results$RMSE_Test, 
               rf_subset$RMSE_Train, rf_subset$RMSE_Test))

# Decision Tree Plot
dt_plot <- ggplot(dt_results) +
  geom_line(aes(x = Depth, y = RMSE_Train, color = "Training"), size = 1.2) +
  geom_point(aes(x = Depth, y = RMSE_Train, color = "Training"), size = 3) +
  geom_line(aes(x = Depth, y = RMSE_Test, color = "Test"), size = 1.2) +
  geom_point(aes(x = Depth, y = RMSE_Test, color = "Test"), size = 3) +
  scale_color_manual(values = c("Test" = "#E74C3C", "Training" = "#3498DB")) +
  ylim(y_min, y_max) +
  labs(
    title = "Decision Trees: Increasing Overfitting",
    subtitle = "Training RMSE decreases while Test RMSE increases",
    x = "Maximum Depth",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 13),
    panel.grid.minor = element_blank()
  )

# Random Forest Plot
rf_plot <- ggplot(rf_subset) +
  geom_line(aes(x = Trees, y = RMSE_Train, color = "Training"), size = 1.2) +
  geom_point(aes(x = Trees, y = RMSE_Train, color = "Training"), size = 3) +
  geom_line(aes(x = Trees, y = RMSE_Test, color = "Test"), size = 1.2) +
  geom_point(aes(x = Trees, y = RMSE_Test, color = "Test"), size = 3) +
  scale_color_manual(values = c("Test" = "#E74C3C", "Training" = "#3498DB")) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000)) +
  ylim(y_min, y_max) +
  labs(
    title = "Random Forests: No Overfitting",
    subtitle = "Both Training and Test RMSE improve together",
    x = "Number of Trees (log scale)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 13),
    panel.grid.minor = element_blank()
  )

# Display plots side by side with aligned Y-axes
grid.arrange(dt_plot, rf_plot, ncol = 2)
```

#### Discussion: Your Analysis Here

Individual decision trees begin to overfit as the become more complex because the model begins to memorize the training data rather than learning the underlying patterns. This is evident in the decision tree plot as the test RMSE increases even as the training RMSE decreases. Further, we can seed the spread between the test and training data increase as the depth increases.

Random forests do not suffer from the same overfitting problem because the model is an ensemble of decision trees, and the errors of the individual trees are averaged out. This is evident in the random forest plot as the test RMSE and training RMSE move together and the spread between them remain relatively constant.

Overall, we can see that the random forest model is much less prone to overfitting than the decision tree model. It overall outperforms the decision tree model in both training and test data.

There are several mechanisms that prevent overfitting in random forests. Bootstrapping trains each tree on a different random sample of the data, reducing correlation between trees.
Random feature selection forces trees to consider different predictors, increasing diversity.
Averaging the predictions smooths out individual errors, preventing any single overfit tree from dominating the result.

### 3. Linear Regression vs Random Forest Comparison

```{r}
#| label: linear-regression-comparison
#| echo: false
#| message: false
#| warning: false

# Train linear regression model
lm_model <- lm(SalePrice ~ ., data = train_data)
lm_pred_train <- predict(lm_model, train_data)
lm_pred_test <- predict(lm_model, test_data)
lm_rmse_train <- sqrt(mean((train_data$SalePrice - lm_pred_train)^2))
lm_rmse_test <- sqrt(mean((test_data$SalePrice - lm_pred_test)^2))
lm_r2 <- 1 - sum((test_data$SalePrice - lm_pred_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Get random forest metrics from performance_df
rf_1_rmse_train <- performance_df$RMSE_Train[performance_df$Trees == 1]
rf_1_rmse_test <- performance_df$RMSE_Test[performance_df$Trees == 1]
rf_1_r2 <- performance_df$R_squared[performance_df$Trees == 1]

rf_100_rmse_train <- performance_df$RMSE_Train[performance_df$Trees == 100]
rf_100_rmse_test <- performance_df$RMSE_Test[performance_df$Trees == 100]
rf_100_r2 <- performance_df$R_squared[performance_df$Trees == 100]

rf_1000_rmse_train <- performance_df$RMSE_Train[performance_df$Trees == 1000]
rf_1000_rmse_test <- performance_df$RMSE_Test[performance_df$Trees == 1000]
rf_1000_r2 <- performance_df$R_squared[performance_df$Trees == 1000]

# Create comparison table
comparison_table <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", 
            "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  Train_RMSE = c(lm_rmse_train, rf_1_rmse_train, rf_100_rmse_train, rf_1000_rmse_train),
  Test_RMSE = c(lm_rmse_test, rf_1_rmse_test, rf_100_rmse_test, rf_1000_rmse_test),
  R_squared = c(lm_r2, rf_1_r2, rf_100_r2, rf_1000_r2),
  Improvement = c("Baseline", 
                  paste0(round((lm_rmse_test - rf_1_rmse_test) / lm_rmse_test * 100, 1), "%"),
                  paste0(round((lm_rmse_test - rf_100_rmse_test) / lm_rmse_test * 100, 1), "%"),
                  paste0(round((lm_rmse_test - rf_1000_rmse_test) / lm_rmse_test * 100, 1), "%"))
)

# Format the RMSE columns with dollar signs and commas
comparison_table$Train_RMSE_fmt <- paste0("$", format(round(comparison_table$Train_RMSE), big.mark = ","))
comparison_table$Test_RMSE_fmt <- paste0("$", format(round(comparison_table$Test_RMSE), big.mark = ","))
comparison_table$R_squared_fmt <- round(comparison_table$R_squared, 3)

# Display the table
library(knitr)
kable(comparison_table[, c("Model", "Train_RMSE_fmt", "Test_RMSE_fmt", "R_squared_fmt", "Improvement")],
      col.names = c("Model", "Training RMSE", "Test RMSE", "R¬≤", "Improvement vs LR"),
      align = c("l", "r", "r", "r", "r"),
      caption = "Performance Comparison: Linear Regression vs Random Forests")
```

#### Discussion: Your Analysis Here

There is a significant improvement in RMSE when going from 1 tree to 100 trees. This is a 40% improvement in RMSE. When switching from linear regression to 100-tree random forest, the improvement is even more significant at 50% improvement in RMSE.

It was surprising to see that the linear regression model performed so well given the complexity and randoness of this real estate dataset. Overall, I learned that a singular decision tree is not an effective model, however when combined with an ensemble of trees, the model becomes much more powerful. It is difficult to say for sure whether random forests are worth the added complexity vs linear regression given the relatively similar performace. However, it depends on the specific dataset at hand and the complexity of the relationships between the variables. It's never a bad idea to try both models and see which one performs better.

